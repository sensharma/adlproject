{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "adlproj_transf_lrn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOhFQytTOlS2iadnwEtFNe8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sensharma/adlproject/blob/main/adlproj_transf_lrn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoIovvxpehoN",
        "outputId": "d6996427-b99c-4502-80c5-d4595e2731ce"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ls /content/gdrive/MyDrive/data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "apple_strawberry.jpg\tdogs.png\t\t   LibriSpeech\n",
            "basket\t\t\tFashionMNIST\t\t   MNIST\n",
            "cifar-10-batches-py\thymenoptera_data\t   text_dataset_test\n",
            "cifar-10-python.tar.gz\timagenet_class_index.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8tHt5QXe1IY"
      },
      "source": [
        "import os\n",
        "import time\n",
        "from copy import deepcopy\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision import datasets, models, transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwnjmuTfe1Er"
      },
      "source": [
        "data_path = os.path.join(os.getcwd(), 'gdrive', 'MyDrive', 'data')\n",
        "models_path = os.path.join(os.getcwd(), 'gdrive', 'MyDrive', 'colabdrive', 'adlproject', 'saved_models')\n",
        "plots_path = os.path.join(os.getcwd(), 'gdrive', 'MyDrive', 'colabdrive', 'adlproject', 'plots', 'MNIST')\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = \"cuda\" if use_cuda else \"cpu\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghrg-myF7olf"
      },
      "source": [
        "print(ten_resnet_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyehNGmKq8gL"
      },
      "source": [
        "### Creating a ResNet18 for training on 10-class FashionMNIST\n",
        "The structure of ResNet is printed below. The transfer learning setup can be done a few ways. Approach here:\n",
        "- Change input Conv2d layer to accept 1-channel input for FashionMNIST\n",
        "- Additional final layer, from 1000 (# out classes in ImageNet, on which it was trained) to 10 (# classes neede for FashionMNIST)\n",
        "- There is no softmax in the model as it was trained with `nn.CrossEntropyLoss` that includes softmax (equivalent to `LogSoftMax` + `NLLLoss`). `CrossEntropyLoss` approach used here.\n",
        "- Data needs resizing, because of shape ResNet takes - done in dataloader with `Resize`, which which uses interpolation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuvpIYGbtVd5",
        "outputId": "81891012-7f8f-416e-ba2c-4831108f8c8c"
      },
      "source": [
        "pret_resnet = models.resnet18(pretrained=True)\n",
        "last_out_features = pret_resnet.fc.out_features\n",
        "conv_struct = ten_resnet_model.pretrained.conv1\n",
        "print(last_out_features, '\\n', conv_struct)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000 \n",
            " Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTEFDf9WZ91i"
      },
      "source": [
        "class TenResNet(nn.Module):\n",
        "    def __init__(self, in_channels=1):\n",
        "        super(TenResNet, self).__init__()\n",
        "        self.model = models.resnet18(pretrained=True)\n",
        "        # in original model\n",
        "        # self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.model.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        out_features = self.model.fc.in_features\n",
        "        self.model.fc = nn.Linear(out_features, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "ten_resnet_model = TenResNet(in_channels=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcGqhmHgrmc2"
      },
      "source": [
        "Freezing weight update for all layers, except the two layers to learn\n",
        "- first input 1-channel convolutional layer\n",
        "- final output layer (10 classes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcCEMOWeb06v"
      },
      "source": [
        "for param in ten_resnet_model.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in ten_resnet_model.model.conv1.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in ten_resnet_model.model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "# for param in ten_resnet_model.parameters():\n",
        "#     print(param.requires_grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcoFtmx_cSYl"
      },
      "source": [
        "Training and Eval functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBBZF2c5hIzQ"
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        # print(output.shape, target.shape)\n",
        "        # loss = F.nll_loss(output, target)\n",
        "        criterion = nn.CrossEntropyLoss().to(device)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    return loss\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            criterion = nn.CrossEntropyLoss().to(device)\n",
        "            test_loss += criterion(output, target).item() # sum up batch loss\n",
        "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))\n",
        "    return correct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTmxBeh2cHCU"
      },
      "source": [
        "Dataloaders - with transforms to match ResNet size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ttl5VeQjG_a5"
      },
      "source": [
        "fm_train_dataset = datasets.FashionMNIST(root=data_path,\n",
        "                                         train=True,\n",
        "                                         download=True,\n",
        "                                         ).data.float()\n",
        "\n",
        "data_transform = transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                     transforms.ToTensor(), \n",
        "                                     transforms.Normalize((fm_train_dataset.mean()/255), \n",
        "                                                          (fm_train_dataset.std()/255)),\n",
        "                                     ])\n",
        "\n",
        "f_mnist_train_loader = DataLoader(\n",
        "    dataset=datasets.FashionMNIST(root=data_path,\n",
        "                                  train=True,\n",
        "                                  download=True,\n",
        "                                  transform=data_transform,\n",
        "                                  ),\n",
        "    batch_size=128,\n",
        "    shuffle=False,\n",
        "    )\n",
        "\n",
        "f_mnist_test_loader = DataLoader(\n",
        "    dataset=datasets.FashionMNIST(root=data_path,\n",
        "                                  train=False,\n",
        "                                  download=True,\n",
        "                                  transform=data_transform,\n",
        "                                  ),\n",
        "    batch_size=128,\n",
        "    shuffle=False,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4Rtkbci8CS5"
      },
      "source": [
        "### Train and save best model\n",
        "\n",
        "Optimiser argument limited to those weights that require update to optimise computation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCK4ic4YNk5Y",
        "outputId": "9dd19daa-36e3-45e8-ce22-862ffa400b20"
      },
      "source": [
        "num_epochs = 20\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, ten_resnet_model.parameters()), lr=3e-4)  #3e-4\n",
        "# optimizer = optim.Adadelta(filter(lambda p: p.requires_grad, ten_resnet_model.parameters()))\n",
        "# optimizer = optim.SGD(filter(lambda p: p.requires_grad, ten_resnet_model.parameters()), lr=0.01, momentum=0.8)\n",
        "\n",
        "model = ten_resnet_model.to(device)\n",
        "\n",
        "best = 0\n",
        "# train and save best model (based on validation accuracy)\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    loss = train(model, device, f_mnist_train_loader, optimizer, epoch)\n",
        "    correct = test(model, device, f_mnist_test_loader)\n",
        "    if correct > best:\n",
        "        best = correct\n",
        "        torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'loss': loss,\n",
        "        }, f'{models_path}/f_mnist_res_cpt_conv_add')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.474890\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.748187\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.576426\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.555430\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.514772\n",
            "\n",
            "Test set: Average loss: 0.0101, Accuracy: 5681/10000 (57%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.259415\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.157058\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.086110\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.258283\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.220882\n",
            "\n",
            "Test set: Average loss: 0.0088, Accuracy: 6242/10000 (62%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.055986\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.073224\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.019972\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 1.118018\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.151669\n",
            "\n",
            "Test set: Average loss: 0.0078, Accuracy: 6637/10000 (66%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.883123\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 1.007477\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.903693\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 1.142488\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.104545\n",
            "\n",
            "Test set: Average loss: 0.0076, Accuracy: 6766/10000 (68%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.944966\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.881536\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.863896\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 1.045096\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 1.031326\n",
            "\n",
            "Test set: Average loss: 0.0073, Accuracy: 6867/10000 (69%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.853214\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.930637\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.823525\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 1.035627\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 1.047434\n",
            "\n",
            "Test set: Average loss: 0.0072, Accuracy: 6899/10000 (69%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.766759\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.851865\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.918250\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 1.124741\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.943772\n",
            "\n",
            "Test set: Average loss: 0.0072, Accuracy: 6892/10000 (69%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.771492\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.762051\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.924901\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.911344\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.892182\n",
            "\n",
            "Test set: Average loss: 0.0070, Accuracy: 6985/10000 (70%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.766779\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.848522\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.853160\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.918743\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.873282\n",
            "\n",
            "Test set: Average loss: 0.0069, Accuracy: 7003/10000 (70%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.798233\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.819879\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.836148\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.929307\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.977044\n",
            "\n",
            "Test set: Average loss: 0.0069, Accuracy: 7077/10000 (71%)\n",
            "\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.793832\n",
            "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.763624\n",
            "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.835732\n",
            "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.985634\n",
            "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.878481\n",
            "\n",
            "Test set: Average loss: 0.0067, Accuracy: 7095/10000 (71%)\n",
            "\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.712742\n",
            "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.665175\n",
            "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.842057\n",
            "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.915305\n",
            "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.935291\n",
            "\n",
            "Test set: Average loss: 0.0067, Accuracy: 7097/10000 (71%)\n",
            "\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.801717\n",
            "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.890080\n",
            "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.891366\n",
            "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.921456\n",
            "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.953722\n",
            "\n",
            "Test set: Average loss: 0.0065, Accuracy: 7167/10000 (72%)\n",
            "\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.740035\n",
            "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.866988\n",
            "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.920046\n",
            "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.884741\n",
            "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.856209\n",
            "\n",
            "Test set: Average loss: 0.0065, Accuracy: 7106/10000 (71%)\n",
            "\n",
            "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.695859\n",
            "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.808166\n",
            "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.740885\n",
            "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.908178\n",
            "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.919825\n",
            "\n",
            "Test set: Average loss: 0.0065, Accuracy: 7152/10000 (72%)\n",
            "\n",
            "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.714215\n",
            "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 0.608511\n",
            "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.800740\n",
            "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 0.835106\n",
            "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.863544\n",
            "\n",
            "Test set: Average loss: 0.0066, Accuracy: 7157/10000 (72%)\n",
            "\n",
            "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.681552\n",
            "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 0.640688\n",
            "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.825091\n",
            "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 0.851997\n",
            "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.929259\n",
            "\n",
            "Test set: Average loss: 0.0063, Accuracy: 7324/10000 (73%)\n",
            "\n",
            "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.672755\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsTv9cKzY2yu"
      },
      "source": [
        "Predicting using one of the saved models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE9VLbUgXvdL"
      },
      "source": [
        "from torch.nn import functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXqP6R_jn7gD",
        "outputId": "35230d70-fcd1-4dc4-d4e2-3bf7fbbcde5e"
      },
      "source": [
        "pt_model_file = f'{models_path}/f_mnist_res_cpt_conv_chng'\n",
        "cpt = torch.load(pt_model_file, map_location=device)\n",
        "print(cpt['epoch'])\n",
        "pt_model = TenResNet(in_channels=1)\n",
        "pt_model.load_state_dict(cpt['model_state_dict'])\n",
        "pt_model.to(device)\n",
        "\n",
        "pt_model.eval()\n",
        "with torch.no_grad():\n",
        "    pred = pt_model(batch[0][0:1].to(device))\n",
        "    probs = torch.exp(F.log_softmax(pred, dim=1))\n",
        "print(probs, torch.sum(probs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18\n",
            "tensor([[7.2196e-05, 4.3660e-07, 9.2756e-06, 3.4555e-04, 4.1377e-05, 3.9823e-03,\n",
            "         2.5928e-05, 1.7179e-02, 3.0979e-03, 9.7525e-01]], device='cuda:0') tensor(1., device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}